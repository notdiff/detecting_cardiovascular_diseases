{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVnWB-3HjwHn"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wandb focal_loss_torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as sts\n",
        "import os\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict, OrderedDict\n",
        "from focal_loss.focal_loss import FocalLoss\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "RXDm2YGFkh9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Подгрузка данных\n",
        "\n"
      ],
      "metadata": {
        "id": "us6h7yvUWXir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1Rt0I7Svrx77tFMCsNubEQ-cDY8hD-iCk #r_peaks.zip\n",
        "!gdown 1GWyzUaz_mOwYDbLuopjroIcfngjSJWkD #labels\n",
        "!unzip r_peaks.zip"
      ],
      "metadata": {
        "id": "guLiPcGn790B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        " 'NORM' : 0,\n",
        " 'IMI': 1,\n",
        " 'NDT': 2,\n",
        " 'NST_': 3,\n",
        " 'LVH': 4,\n",
        " 'LAFB': 5,\n",
        " 'IRBBB': 6,\n",
        " 'IVCD': 7,\n",
        " 'ASMI': 8,\n",
        " 'AMI': 9,\n",
        " 'ISCAL': 10,\n",
        " '1AVB': 11,\n",
        " 'ILMI': 12,\n",
        " 'ISC_': 13,\n",
        " 'CRBBB': 14,\n",
        " 'CLBBB': 15,\n",
        " 'LAO/LAE': 16}\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "tKzklbFrU-WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Разделение датасета\n",
        "Для одного target_class, все остальные классы обозначаются как 0"
      ],
      "metadata": {
        "id": "_5STPLkohteR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.read_csv(\"train_val_labels.csv\")\n",
        "#в target_class номер класса для обучения одной из сетей\n",
        "target_class = 8\n",
        "left_classes = [i for i in labels.result_class.unique() if i != target_class]\n",
        "num_others = (len(labels[labels.result_class == target_class]) * 2) // 15\n",
        "data = labels[labels.result_class == target_class]\n",
        "data.loc[:, [\"result_class\"]] = 1\n",
        "data.index = range(0, len(data))\n",
        "for cur_class in left_classes:\n",
        "  cur_class_data = labels[(labels.result_class == cur_class)]\n",
        "  cur_class_data = cur_class_data[~cur_class_data.record_name.isin(labels[labels.result_class != cur_class].record_name)]\n",
        "  cur_frame = cur_class_data.sample(n=min(len(cur_class_data), num_others))\n",
        "  cur_frame.loc[:, [\"result_class\"]] = 0\n",
        "  data = pd.concat([data, cur_frame], axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeFOFly7_a4w",
        "outputId": "532ab038-be70-4318-f02c-4aceda3bd49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-62396d917742>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data.loc[:, [\"result_class\"]] = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "CJtOW5cAxJEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EcgPTBDataset(Dataset):\n",
        "    def __init__(self, labels, path='/'):\n",
        "        self.x_paths = [labels.iloc[i, 0] for i in range(len(labels))]\n",
        "        self.labels = [labels.iloc[i, 1] for i in range(len(labels))]\n",
        "        self.path = path\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        hr = torch.tensor(np.load(self.path + self.x_paths[idx] + '.npy'))[None, :, :]\n",
        "\n",
        "        target = self.labels[idx]\n",
        "\n",
        "        return hr, target"
      ],
      "metadata": {
        "id": "ff8S4sfZk7b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ptb_set = EcgPTBDataset(data, path=\"/content/r_peaks/signals/\")\n",
        "\n",
        "valid_data, train_data = random_split(ptb_set, lengths=[0.1, 0.9])\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=1)\n",
        "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=1)"
      ],
      "metadata": {
        "id": "1UEWfz3elL0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ECGNET"
      ],
      "metadata": {
        "id": "1V_ARXu4kbny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ECGNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ECGNet, self).__init__()\n",
        "    #layer1\n",
        "    self.layer1_conv2d = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(1, 25), stride=(1, 2), bias=True)\n",
        "\n",
        "\n",
        "    #layer2\n",
        "    self.layer2_conv2d = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm2d(num_features=32)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv2d(32, 64, kernel_size=(1, 15), stride=(1, 1), bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv2d(64, 64, kernel_size=(1, 15), stride=(1, 2),  bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv2d(64, 32, kernel_size=(1, 15), stride=(1, 1), bias=True)),\n",
        "    ]))\n",
        "    self.layer2_seModule = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv2d(32, 16, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv2d(16, 32, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    #layer3\n",
        "    self.layer3_conv2d_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm2d(num_features=32)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv2d(32, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv2d(64, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=True)),\n",
        "    ]))\n",
        "    self.layer3_seModule_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv2d(32, 16, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv2d(16, 32, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer3_conv2d_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm2d(num_features=32)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv2d(32, 64, kernel_size=(5, 1), padding=(2, 0), bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv2d(64, 64, kernel_size=(5, 1), padding=(2, 0), bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv2d(64, 32, kernel_size=(5, 1), padding=(2, 0), bias=True)),\n",
        "    ]))\n",
        "    self.layer3_seModule_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv2d(32, 16, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv2d(16, 32, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer3_conv2d_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm2d(num_features=32)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv2d(32, 64, kernel_size=(7, 1), padding=(3, 0), bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv2d(64, 64, kernel_size=(7, 1), padding=(3, 0), bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm2d(num_features=64)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv2d(64, 32, kernel_size=(7, 1), padding=(3, 0), bias=True)),\n",
        "    ]))\n",
        "    self.layer3_seModule_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv2d(32, 16, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv2d(16, 32, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    #layer4\n",
        "    self.layer4_conv1d_short_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 384, kernel_size=3, stride=9, bias=True)),\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 768, kernel_size=3, stride=2, bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv1d(768, 768, kernel_size=3, stride=1, bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv1d(768, 1536, kernel_size=3, stride=2, bias=True)),\n",
        "        (\"bn4\", nn.BatchNorm1d(num_features=1536)),\n",
        "        (\"act4\", nn.ReLU()),\n",
        "        (\"cn4\", nn.Conv1d(1536, 384, kernel_size=3, stride=2, bias=True)),\n",
        "    ]))\n",
        "    self.layer4_seModule_block1 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv1d(384, 48, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv1d(48, 384, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_short_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 384, kernel_size=5, stride=9, bias=True)),\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 768, kernel_size=5, stride=2, padding=2, bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv1d(768, 768, kernel_size=5, stride=2, padding=1, bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv1d(768, 1536, kernel_size=5, stride=1, padding=2, bias=True)),\n",
        "        (\"bn4\", nn.BatchNorm1d(num_features=1536)),\n",
        "        (\"act4\", nn.ReLU()),\n",
        "        (\"cn4\", nn.Conv1d(1536, 384, kernel_size=5, stride=2, padding=1, bias=True)),\n",
        "    ]))\n",
        "    self.layer4_seModule_block2 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv1d(384, 48, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv1d(48, 384, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_short_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 384, kernel_size=7, stride=9, bias=True)),\n",
        "    ]))\n",
        "\n",
        "    self.layer4_conv1d_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"bn1\", nn.BatchNorm1d(num_features=384)),\n",
        "        (\"act1\", nn.ReLU()),\n",
        "        (\"cn1\", nn.Conv1d(384, 768, kernel_size=7, stride=2, padding=2, bias=True)),\n",
        "        (\"bn2\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act2\", nn.ReLU()),\n",
        "        (\"cn2\", nn.Conv1d(768, 768, kernel_size=7, stride=2, padding=1, bias=True)),\n",
        "        (\"bn3\", nn.BatchNorm1d(num_features=768)),\n",
        "        (\"act3\", nn.ReLU()),\n",
        "        (\"cn3\", nn.Conv1d(768, 1536, kernel_size=7, stride=1, padding=3, bias=True)),\n",
        "        (\"bn4\", nn.BatchNorm1d(num_features=1536)),\n",
        "        (\"act4\", nn.ReLU()),\n",
        "        (\"cn4\", nn.Conv1d(1536, 384, kernel_size=7, stride=2, padding=2, bias=True)),\n",
        "    ]))\n",
        "    self.layer4_seModule_block3 = nn.Sequential(OrderedDict([\n",
        "        (\"fc1\", nn.Conv1d(384, 48, kernel_size=1, bias=True)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"fc2\", nn.Conv1d(48, 384, kernel_size=1, bias=True)),\n",
        "        (\"gate\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "    self.layer5_avg_pool1 = nn.AvgPool1d(kernel_size=10)\n",
        "    self.layer5_avg_pool2 = nn.AvgPool1d(kernel_size=10)\n",
        "    self.layer5_avg_pool3 = nn.AvgPool1d(kernel_size=10)\n",
        "\n",
        "    self.fc = nn.Sequential(OrderedDict([\n",
        "        (\"ln1\", nn.Linear(1152, 288)),\n",
        "        (\"dp\", nn.Dropout(p=0.2)),\n",
        "        (\"act\", nn.ReLU()),\n",
        "        (\"ln2\", nn.Linear(288, 1)),\n",
        "        (\"sigmoid\", nn.Sigmoid())\n",
        "    ]))\n",
        "\n",
        "  def forward(self, x, embeds=False):\n",
        "    #layer1\n",
        "    x = self.layer1_conv2d(x)\n",
        "\n",
        "    #layer2\n",
        "    x = self.layer2_conv2d(x)\n",
        "    u = x\n",
        "    x = x.view(x.size(0), x.size(1), -1).mean(-1).view(x.size(0), x.size(1), 1, 1)\n",
        "    x = self.layer2_seModule(x)\n",
        "    x = u * x\n",
        "\n",
        "    #layer3\n",
        "    x1 = self.layer3_conv2d_block1(x)\n",
        "    u1 = x1\n",
        "    x1 = x1.view(x1.size(0), x1.size(1), -1).mean(-1).view(x1.size(0), x1.size(1), 1, 1)\n",
        "    x1 = self.layer3_seModule_block1(x1)\n",
        "    x1 = u1 * x1\n",
        "\n",
        "    x2 = self.layer3_conv2d_block2(x)\n",
        "    u2 = x2\n",
        "    x2 = x2.view(x2.size(0), x2.size(1), -1).mean(-1).view(x2.size(0), x2.size(1), 1, 1)\n",
        "    x2 = self.layer3_seModule_block2(x2)\n",
        "    x2 = u2 * x2\n",
        "\n",
        "    x3 = self.layer3_conv2d_block3(x)\n",
        "    u3 = x3\n",
        "    x3 = x3.view(x3.size(0), x3.size(1), -1).mean(-1).view(x3.size(0), x3.size(1), 1, 1)\n",
        "    x3 = self.layer3_seModule_block3(x3)\n",
        "    x3 = u3 * x3\n",
        "\n",
        "    #layer4\n",
        "    x1 = torch.flatten(x1, start_dim=1, end_dim=2)\n",
        "    x2 = torch.flatten(x2, start_dim=1, end_dim=2)\n",
        "    x3 = torch.flatten(x3, start_dim=1, end_dim=2)\n",
        "\n",
        "    # x1 = x1.unsqueeze(1)\n",
        "    # x2 = x2.unsqueeze(1)\n",
        "    # x3 = x3.unsqueeze(1)\n",
        "\n",
        "    x1_short = self.layer4_conv1d_short_block1(x1)\n",
        "\n",
        "    x1 = self.layer4_conv1d_block1(x1)\n",
        "    u1 = x1\n",
        "    x1 = x1.view(x1.size(0), x1.size(1), -1).mean(-1).view(x1.size(0), x1.size(1), 1, 1).flatten(2, 3)\n",
        "    x1 = self.layer4_seModule_block1(x1)\n",
        "    x1 = u1 * x1\n",
        "    x1 = x1 + x1_short\n",
        "\n",
        "    x2_short = self.layer4_conv1d_short_block2(x2)\n",
        "\n",
        "    x2 = self.layer4_conv1d_block2(x2)\n",
        "    u2 = x2\n",
        "    x2 = x2.view(x2.size(0), x2.size(1), -1).mean(-1).view(x2.size(0), x2.size(1), 1, 1).flatten(2, 3)\n",
        "    x2 = self.layer4_seModule_block2(x2)\n",
        "    x2 = u2 * x2\n",
        "    x2 = x2 + x2_short\n",
        "\n",
        "    x3_short = self.layer4_conv1d_short_block3(x3)\n",
        "\n",
        "    x3 = self.layer4_conv1d_block3(x3)\n",
        "    u3 = x3\n",
        "    x3 = x3.view(x3.size(0), x3.size(1), -1).mean(-1).view(x3.size(0), x3.size(1), 1, 1).flatten(2, 3)\n",
        "    x3 = self.layer4_seModule_block3(x3)\n",
        "    x3 = u3 * x3\n",
        "    x3 = x3 + x3_short\n",
        "\n",
        "    x1 = self.layer5_avg_pool1(x1)\n",
        "    x2 = self.layer5_avg_pool2(x2)\n",
        "    x3 = self.layer5_avg_pool3(x3)\n",
        "\n",
        "    x = torch.cat((x1, x2, x3), dim=1).flatten(1)\n",
        "\n",
        "    if embeds:\n",
        "        return self.fc.ln1(x)\n",
        "\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "r1XSNivRkaEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "3mC8KeqmxMsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(output, target):\n",
        "    train_accuracy = torch.sum(target == output) / len(target)\n",
        "    return train_accuracy\n",
        "\n",
        "def calculate_f1(preds, labels):\n",
        "    tp = torch.sum(preds[labels == preds] == 1)\n",
        "    preds_p = torch.sum(preds == 1)\n",
        "    labels_p = torch.sum(labels == 1)\n",
        "    recall = (tp / labels_p if labels_p != 0 else 0)\n",
        "    precision = (tp / preds_p if preds_p != 0 else 0)\n",
        "    if recall + precision == 0: return 0\n",
        "    return (2 * recall * precision) / (recall + precision)\n",
        "\n",
        "class MetricMonitor:\n",
        "    def __init__(self, float_precision=3):\n",
        "        self.float_precision = float_precision\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
        "\n",
        "    def update(self, metric_name, val):\n",
        "        metric = self.metrics[metric_name]\n",
        "\n",
        "        metric[\"val\"] += val\n",
        "        metric[\"count\"] += 1\n",
        "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \" | \".join(\n",
        "            [\n",
        "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
        "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
        "                )\n",
        "                for (metric_name, metric) in self.metrics.items()\n",
        "            ]\n",
        "        )"
      ],
      "metadata": {
        "id": "iI2rCig7NtIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train/Valid part"
      ],
      "metadata": {
        "id": "w7gMEMVYxPy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch, device):\n",
        "    metric_monitor = MetricMonitor(float_precision=4)\n",
        "    model.train()\n",
        "    stream = tqdm(train_loader)\n",
        "    for i, batch in enumerate(stream, start=1):\n",
        "        x_batch, y_batch = batch\n",
        "        y_batch = y_batch.to(device, non_blocking=True)\n",
        "        x_batch = x_batch.to(device, non_blocking=True)\n",
        "        output = model(x_batch.float()).view(1, -1)[0]\n",
        "        loss = criterion(output, y_batch.float())\n",
        "        output = (output > 0.5).to(torch.int32)\n",
        "        accuracy = calculate_accuracy(output, y_batch)\n",
        "        f1 = calculate_f1(output, y_batch)\n",
        "        metric_monitor.update(\"Loss\", loss)\n",
        "        metric_monitor.update(\"Accuracy\", accuracy)\n",
        "        metric_monitor.update(\"F1\", f1)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        stream.set_description(\n",
        "            \"Epoch: {epoch}. Train.  {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
        "        )"
      ],
      "metadata": {
        "id": "PdFtCcuhn9k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(val_loader, model, criterion, epoch, device):\n",
        "    metric_monitor = MetricMonitor(float_precision=4)\n",
        "    model.eval()\n",
        "    stream = tqdm(val_loader)\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(stream, start=1):\n",
        "            x_batch, y_batch = batch\n",
        "            y_batch = y_batch.to(device, non_blocking=True)\n",
        "            x_batch = x_batch.to(device, non_blocking=True)\n",
        "            output = model(x_batch.float()).view(1, -1)[0]\n",
        "            loss = criterion(output, y_batch.float())\n",
        "            output = (output > 0.5).to(torch.int32)\n",
        "            accuracy = calculate_accuracy(output, y_batch)\n",
        "            f1 = calculate_f1(output, y_batch)\n",
        "            metric_monitor.update(\"Loss\", loss)\n",
        "            metric_monitor.update(\"Accuracy\", accuracy)\n",
        "            metric_monitor.update(\"F1\", f1)\n",
        "            stream.set_description(\n",
        "                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor)\n",
        "            )\n",
        "    return metric_monitor.metrics[\"F1\"][\"avg\"], metric_monitor.metrics[\"Accuracy\"][\"avg\"], metric_monitor.metrics[\"Loss\"][\"avg\"]"
      ],
      "metadata": {
        "id": "ACGdfJ3loAJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ECGNet()\n",
        "model = model.to(device)\n",
        "\n",
        "learning_rate = 3e-5\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
        "\n",
        "loss_fn = FocalLoss(gamma=1.8)"
      ],
      "metadata": {
        "id": "lMyWw32WmVGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login #ключ api wandb\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    project=\"Ecg_one_vs_rest\",\n",
        "\n",
        "    config={\n",
        "        \"architecture\": \"ecg_net\",\n",
        "        \"dataset\": \"r_peaks\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "iO5EGQt51uk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjYwjqn3Pgbq",
        "outputId": "1d6bbc0d-e272-4c2e-f766-3d22f1fbcab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "max_f1 = 0.85\n",
        "for epoch in range(num_epochs):\n",
        "  train(train_loader, model, loss_fn, optimizer, epoch, device)\n",
        "  f1_v, acc_v, loss_v = validate(valid_loader, model, loss_fn, epoch, device)\n",
        "  scheduler.step(f1_v)\n",
        "  wandb.log({\"F1\": f1_v, \"Acc\": acc_v, 'loss': loss_v})\n",
        "  if f1_v > max_f1:\n",
        "    max_f1 = f1_v\n",
        "    torch.save(model.state_dict(), f'/content/drive/MyDrive/models/{f1_v}.pth')"
      ],
      "metadata": {
        "id": "KqDyEpvez6eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Примера создания Catboost"
      ],
      "metadata": {
        "id": "hqv3gZShhESK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#извлечение фич\n",
        "epsn = 1e-8\n",
        "\n",
        "def mean_fea(a):\n",
        "\treturn np.mean(a)\n",
        "\n",
        "def rms_fea(a):\n",
        "\treturn np.sqrt(np.mean(np.square(a)))\n",
        "\n",
        "def sr_fea(a):\n",
        "\treturn np.square(np.mean(np.sqrt(np.abs(a))))\n",
        "\n",
        "def am_fea(a):\n",
        "\treturn np.mean(np.abs(a))\n",
        "\n",
        "def skew_fea(a):\n",
        "\treturn np.mean((a-mean_fea(a))**3)\n",
        "\n",
        "def kurt_fea(a):\n",
        "\treturn np.mean((a-mean_fea(a))**4)\n",
        "\n",
        "def max_fea(a):\n",
        "\treturn np.max(a)\n",
        "\n",
        "def min_fea(a):\n",
        "\treturn np.min(a)\n",
        "\n",
        "def pp_fea(a):\n",
        "\treturn max_fea(a)-min_fea(a)\n",
        "\n",
        "def var_fea(a):\n",
        "\tn = len(a)\n",
        "\treturn np.sum((a-mean_fea(a))**2)/(n-1)\n",
        "\n",
        "def waveform_index(a):\n",
        "\treturn rms_fea(a)/(am_fea(a)+epsn)\n",
        "\n",
        "def peak_index(a):\n",
        "\treturn max_fea(a)/(rms_fea(a)+epsn)\n",
        "\n",
        "def impluse_factor(a):\n",
        "\treturn max_fea(a)/(am_fea(a)+epsn)\n",
        "\n",
        "def tolerance_index(a):\n",
        "\treturn max_fea(a)/(sr_fea(a)+epsn)\n",
        "\n",
        "def skew_index(a):\n",
        "\tn = len(a)\n",
        "\ttemp1 = np.sum((a-mean_fea(a))**3)\n",
        "\ttemp2 = (np.sqrt(var_fea(a)))**3\n",
        "\treturn temp1/((n-1)*temp2)\n",
        "\n",
        "def kurt_index(a):\n",
        "\tn = len(a)\n",
        "\ttemp1 = np.sum((a-mean_fea(a))**4)\n",
        "\ttemp2 = (np.sqrt(var_fea(a)))**4\n",
        "\treturn temp1/((n-1)*temp2)\n",
        "\n",
        "def fft_fft(sequence_data):\n",
        "\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\tdc = fft_trans[0]\n",
        "\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\tfreq_sum_ = np.sum(freq_spectrum)\n",
        "\treturn dc, freq_spectrum, freq_sum_\n",
        "\n",
        "def fft_mean(sequence_data):\n",
        "\tdef fft_fft(sequence_data):\n",
        "\t\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\t\t# dc = fft_trans[0]\n",
        "\t\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\t\t_freq_sum_ = np.sum(freq_spectrum)\n",
        "\t\treturn freq_spectrum, _freq_sum_\n",
        "\tfreq_spectrum, _freq_sum_ = fft_fft(sequence_data)\n",
        "\treturn np.mean(freq_spectrum)\n",
        "\n",
        "def fft_var(sequence_data):\n",
        "\tdef fft_fft(sequence_data):\n",
        "\t\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\t\t# dc = fft_trans[0]\n",
        "\t\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\t\t_freq_sum_ = np.sum(freq_spectrum)\n",
        "\t\treturn freq_spectrum, _freq_sum_\n",
        "\tfreq_spectrum, _freq_sum_ = fft_fft(sequence_data)\n",
        "\treturn np.var(freq_spectrum)\n",
        "\n",
        "def fft_std(sequence_data):\n",
        "\tdef fft_fft(sequence_data):\n",
        "\t\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\t\t# dc = fft_trans[0]\n",
        "\t\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\t\t_freq_sum_ = np.sum(freq_spectrum)\n",
        "\t\treturn freq_spectrum, _freq_sum_\n",
        "\tfreq_spectrum, _freq_sum_ = fft_fft(sequence_data)\n",
        "\treturn np.std(freq_spectrum)\n",
        "\n",
        "def fft_std2(sequence_data):\n",
        "\tdef fft_fft(sequence_data):\n",
        "\t\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\t\t# dc = fft_trans[0]\n",
        "\t\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\t\t_freq_sum_ = np.sum(freq_spectrum)\n",
        "\t\treturn freq_spectrum, _freq_sum_\n",
        "\tfreq_spectrum, _freq_sum_ = fft_fft(sequence_data)\n",
        "\treturn np.std(freq_spectrum)\n",
        "\n",
        "def fft_entropy(sequence_data):\n",
        "\tdef fft_fft(sequence_data):\n",
        "\t\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\t\t# dc = fft_trans[0]\n",
        "\t\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\t\t_freq_sum_ = np.sum(freq_spectrum)\n",
        "\t\treturn freq_spectrum, _freq_sum_\n",
        "\tfreq_spectrum, _freq_sum_ = fft_fft(sequence_data)\n",
        "\tpr_freq = freq_spectrum * 1.0 / _freq_sum_\n",
        "\tentropy = -1 * np.sum([np.log2(p+1e-5) * p for p in pr_freq])\n",
        "\treturn entropy\n",
        "\n",
        "def fft_energy(sequence_data):\n",
        "\tdef fft_fft(sequence_data):\n",
        "\t\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\t\t# dc = fft_trans[0]\n",
        "\t\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\t\t_freq_sum_ = np.sum(freq_spectrum)\n",
        "\t\treturn freq_spectrum, _freq_sum_\n",
        "\tfreq_spectrum, _freq_sum_ = fft_fft(sequence_data)\n",
        "\treturn np.sum(freq_spectrum ** 2) / len(freq_spectrum)\n",
        "\n",
        "def fft_skew(sequence_data):\n",
        "\tdef fft_fft(sequence_data):\n",
        "\t\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\t\t# dc = fft_trans[0]\n",
        "\t\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\t\t_freq_sum_ = np.sum(freq_spectrum)\n",
        "\t\treturn freq_spectrum, _freq_sum_\n",
        "\tfreq_spectrum, _freq_sum_ = fft_fft(sequence_data)\n",
        "\t_fft_mean, _fft_std = fft_mean(sequence_data), fft_std(sequence_data)\n",
        "\treturn np.mean([0 if _fft_std < epsn else np.power((x - _fft_mean) / _fft_std, 3)\n",
        "\t\t\t\t\tfor x in freq_spectrum])\n",
        "\n",
        "def fft_kurt(sequence_data):\n",
        "\tdef fft_fft(sequence_data):\n",
        "\t\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\t\t# dc = fft_trans[0]\n",
        "\t\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\t\t_freq_sum_ = np.sum(freq_spectrum)\n",
        "\t\treturn freq_spectrum, _freq_sum_\n",
        "\tfreq_spectrum, _freq_sum_ = fft_fft(sequence_data)\n",
        "\t_fft_mean, _fft_std = fft_mean(sequence_data), fft_std(sequence_data)\n",
        "\treturn np.mean([0 if _fft_std < epsn else np.power((x - _fft_mean) / _fft_std, 4)\n",
        "\t\t\t\t\tfor x in freq_spectrum])\n",
        "\n",
        "def fft_shape_mean(sequence_data):\n",
        "\tdef fft_fft(sequence_data):\n",
        "\t\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\t\t# dc = fft_trans[0]\n",
        "\t\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\t\t_freq_sum_ = np.sum(freq_spectrum)\n",
        "\t\treturn freq_spectrum, _freq_sum_\n",
        "\tfreq_spectrum, _freq_sum_ = fft_fft(sequence_data)\n",
        "\tshape_sum = np.sum([x * freq_spectrum[x]\n",
        "\t\t\t\t\t\tfor x in range(len(freq_spectrum))])\n",
        "\treturn 0 if _freq_sum_ < epsn else shape_sum * 1.0 / _freq_sum_\n",
        "\n",
        "def fft_shape_std(sequence_data):\n",
        "\tdef fft_fft(sequence_data):\n",
        "\t\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\t\t# dc = fft_trans[0]\n",
        "\t\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\t\t_freq_sum_ = np.sum(freq_spectrum)\n",
        "\t\treturn freq_spectrum, _freq_sum_\n",
        "\tfreq_spectrum, _freq_sum_ = fft_fft(sequence_data)\n",
        "\tshape_mean = fft_shape_mean(sequence_data)\n",
        "\tvar = np.sum([0 if _freq_sum_ < epsn else np.power((x - shape_mean), 2) * freq_spectrum[x]\n",
        "\t\t\t\t  for x in range(len(freq_spectrum))]) / _freq_sum_\n",
        "\treturn np.sqrt(var)\n",
        "\n",
        "def fft_shape_skew(sequence_data):\n",
        "\tdef fft_fft(sequence_data):\n",
        "\t\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\t\t# dc = fft_trans[0]\n",
        "\t\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\t\t_freq_sum_ = np.sum(freq_spectrum)\n",
        "\t\treturn freq_spectrum, _freq_sum_\n",
        "\tfreq_spectrum, _freq_sum_ = fft_fft(sequence_data)\n",
        "\tshape_mean = fft_shape_mean(sequence_data)\n",
        "\treturn np.sum([np.power((x - shape_mean), 3) * freq_spectrum[x]\n",
        "\t\t\t\t   for x in range(len(freq_spectrum))]) / _freq_sum_\n",
        "\n",
        "def fft_shape_kurt(sequence_data):\n",
        "\tdef fft_fft(sequence_data):\n",
        "\t\tfft_trans = np.abs(np.fft.fft(sequence_data))\n",
        "\t\t# dc = fft_trans[0]\n",
        "\t\tfreq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
        "\t\t_freq_sum_ = np.sum(freq_spectrum)\n",
        "\t\treturn freq_spectrum, _freq_sum_\n",
        "\tfreq_spectrum, _freq_sum_ = fft_fft(sequence_data)\n",
        "\tshape_mean = fft_shape_mean(sequence_data)\n",
        "\treturn np.sum([np.power((x - shape_mean), 4) * freq_spectrum[x] - 3\n",
        "\t\t\t\t   for x in range(len(freq_spectrum))]) / _freq_sum_"
      ],
      "metadata": {
        "id": "cyMp09D5iXoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#агрегация всех фич\n",
        "def count_time_features(data):\n",
        "\n",
        "  result_mean_fea = mean_fea(data)\n",
        "  result_rms_fea = rms_fea(data)\n",
        "  result_sr_fea = sr_fea(data)\n",
        "  result_am_fea = am_fea(data)\n",
        "  result_skew_fea = skew_fea(data)\n",
        "  result_kurt_fea = kurt_fea(data)\n",
        "  result_max_fea = max_fea(data)\n",
        "  result_min_fea = min_fea(data)\n",
        "  result_pp_fea = pp_fea(data)\n",
        "  result_var_fea = var_fea(data)\n",
        "  result_waveform_index_fea = waveform_index(data)\n",
        "  result_peak_index_fea = peak_index(data)\n",
        "  result_impluse_factor_fea = impluse_factor(data)\n",
        "\n",
        "  return np.array([\n",
        "      result_mean_fea,\n",
        "      result_rms_fea,\n",
        "      result_var_fea,\n",
        "      result_waveform_index_fea,\n",
        "      result_peak_index_fea,\n",
        "      result_sr_fea,\n",
        "      result_am_fea,\n",
        "      result_skew_fea,\n",
        "      result_kurt_fea,\n",
        "      result_max_fea,\n",
        "      result_min_fea,\n",
        "      result_pp_fea,\n",
        "      result_impluse_factor_fea\n",
        "  ])\n",
        "\n",
        "def count_freq_features(data):\n",
        "\n",
        "  result_fft_mean = fft_mean(data)\n",
        "  result_fft_var = fft_var(data)\n",
        "  result_fft_std = fft_std(data)\n",
        "  result_fft_entropy = fft_entropy(data)\n",
        "  result_fft_energy = fft_energy(data)\n",
        "  result_fft_skew = fft_skew(data)\n",
        "  result_fft_kurt = fft_kurt(data)\n",
        "  result_fft_shape_mean = fft_shape_mean(data)\n",
        "  result_fft_shape_std = fft_shape_std(data)\n",
        "\n",
        "  return np.array([\n",
        "      result_fft_mean,\n",
        "      result_fft_var,\n",
        "      result_fft_std,\n",
        "      result_fft_entropy,\n",
        "      result_fft_energy,\n",
        "      result_fft_skew,\n",
        "      result_fft_kurt,\n",
        "      result_fft_shape_mean,\n",
        "      result_fft_shape_std\n",
        "  ])\n",
        "\n",
        "def get_all_features(ecg_signal):\n",
        "\n",
        "\n",
        "    tmp_raw = np.array(ecg_signal)\n",
        "    time_features_tmp=[]\n",
        "    fre_features_tmp=[]\n",
        "\n",
        "    for j in range(0,12):\n",
        "\n",
        "        tmp_lead = tmp_raw[j]\n",
        "        result_time_features = count_time_features(tmp_lead)\n",
        "        result_fre_features = count_freq_features(tmp_lead)\n",
        "        time_features_tmp.append(result_time_features)\n",
        "        fre_features_tmp.append(result_fre_features)\n",
        "\n",
        "\n",
        "    return np.array(time_features_tmp), np.array(fre_features_tmp)"
      ],
      "metadata": {
        "id": "1BNF4we0hrag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_signal_embed(signal, model, device = torch.device(\"cuda\")):\n",
        "\n",
        "  model.eval()\n",
        "  model = model.to(device)\n",
        "  signal = signal.to(device)\n",
        "  with torch.no_grad():\n",
        "    pred_by_model = model(signal.float().unsqueeze(0))\n",
        "    embeds = model.get_embeds(signal.float().unsqueeze(0))\n",
        "\n",
        "  return np.array(embeds.cpu())"
      ],
      "metadata": {
        "id": "XO2LKcAQgpx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_paths = [data.iloc[i, 0] for i in range(len(data))]\n",
        "labels = [data.iloc[i, 1] for i in range(len(data))]\n",
        "root_path = \"/content/r_peaks/signals/\""
      ],
      "metadata": {
        "id": "GRYciH01hWr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#создание датасета для catboost на основе всех эмбеддингов ECGNet\n",
        "cb_dataset = np.zeros((1, 553), dtype=np.int16)\n",
        "\n",
        "for x_path, label in zip(tqdm(x_paths), labels):\n",
        "  hr = torch.tensor(np.load(root_path + x_path + '.npy'))[None, :, :]\n",
        "  t_f, f_f = get_all_features(hr[0])\n",
        "  features = np.append(t_f, f_f)\n",
        "  X = get_signal_embed(hr, models_dict[0][\"model\"], torch.device(\"cuda\"))\n",
        "  # print(X.shape, features.shape)\n",
        "  X = np.append(X, features)\n",
        "  cb_dataset = np.append(cb_dataset, np.append(X, np.array(label)).reshape(1, -1))"
      ],
      "metadata": {
        "id": "kb6J3mzLhYv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb_dataset_reshaped = np.reshape(cb_dataset, (-1, 553))\n",
        "cb_dataset_reshaped = pd.DataFrame(cb_dataset_reshaped, columns=[[f\"e_{i}\" for i in range(0, 288)] + [f\"f_{i}\" for i in range(0, 264)] + [\"target\"]])\n",
        "\n",
        "cb_dataset_reshaped = cb_dataset_reshaped.drop(index=[0])"
      ],
      "metadata": {
        "id": "-U4CP9mMhbbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y707IccfhfOR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}